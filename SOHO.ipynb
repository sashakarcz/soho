{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb57df9-031a-4913-8de0-eb379d9ac474",
   "metadata": {},
   "source": [
    "# SOHO photometry\n",
    "This Jupyter Notebook is built to assist with creating publication quality plots for SOHO data of comets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7767714-9dd0-4294-9cb2-f449094716b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Button, FileUpload, widgets\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd687e9a-7122-4b16-9f5c-b69e77d73624",
   "metadata": {},
   "source": [
    "## Quickstart\n",
    "1. Upload the MPC data file with the file extension .txt\n",
    "2. Upload the SOHO data file with the file extension .dat\n",
    "3. Update the `variables` in the second code cell\n",
    "4. Press the `Restart the kernel and run all cells` button. It looks like the fast forward button (>>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8361687-b2a8-41f3-9a18-49887165752a",
   "metadata": {},
   "source": [
    "## Variables\n",
    "<a id=\"variables\"></a>\n",
    "All the variables that you may want to use are defined here. You only need to change the next code cell's values. Once that is complete, press the `Restart the kernel and run all cells` -- this looks like a fast forward button (>>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8b2ef3-d99f-4451-b572-2d78a73eed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All variables should go here\n",
    "\n",
    "COLUMN_TO_PLOT='m03' # Header value for the column you want to graph\n",
    "COMET='210P/Christensen' # Title to put in Graph\n",
    "DF1_LABEL='MPC' # Data legned value for MPC data\n",
    "DF2_LABEL='SOHO' # Data legend value for SOHO data, if empty will not create graph legend.\n",
    "ERROR_COLUMN='m03err' # Error bars for the measured SOHO data. Default is m03err, if not present, this will graph without error bars.\n",
    "MAX_MAG=10 # Adjust the Y-Axis for max magnitude\n",
    "MIN_MAG=3 # Adjust the Y-Axis for min magnitude\n",
    "SOHO_ONLY=True # If set to True, it will only plot SOHO data, and update the X-Axis tick marks to include HH:MM, if False standard\n",
    "XLABEL='Date' # This is what the main label of the X-Axis is\n",
    "XROT=35 # Number of degrees to rotate the tick labels\n",
    "XSTEPS=170 # This is the number of ticks on the X-axis\n",
    "YEAR = '2008' # Year of observation\n",
    "YLABEL='Magnitude'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c91a25-a790-4869-9180-8c15e530a6e9",
   "metadata": {},
   "source": [
    "## Hidden Cells\n",
    "Most of the cells in this notebook are hidden / collapsed. Click the bar to the left of each cell to expand / collapse them. They are hidden out of attention to user experience. You may edit any value, or add any steps in any of the code cells. We have tried to be verbose in our comments to make it easier to follow what is happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a206b044-af47-4569-8c78-dd0bf65145aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# List all files in the current directory\n",
    "files = os.listdir()\n",
    "\n",
    "# Filter txt files for df1 and dat files for df2\n",
    "df1_file = next((file for file in files if file.endswith('.txt')), None)\n",
    "df2_file = next((file for file in files if file.endswith('.dat')), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d37aaa-ac31-43f3-bf24-01ee5dcdbb0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Enable LaTeX text rendering and 300 DPI\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Computer Modern\"],\n",
    "    \"figure.dpi\": \"300\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f8dc1c6-221b-4670-ad49-06d6517fae70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read the first data file and add columns 1,2,3,10 (zero indexed) to the pd object with index of Year, Month, Day, Magnitude\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMonth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMagnitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:472\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(filepath_or_buffer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filepath_or_buffer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    470\u001b[0m ):\n\u001b[1;32m    471\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid file path or buffer object type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(filepath_or_buffer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m IOArgs(\n\u001b[1;32m    475\u001b[0m     filepath_or_buffer\u001b[38;5;241m=\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    476\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    480\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "# Read the first data file and add columns 1,2,3,10 (zero indexed) to the pd object with index of Year, Month, Day, Magnitude\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    clean_lines = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "            if len(parts) < 13:\n",
    "                parts.extend(['NaN'] * (13 - len(parts)))\n",
    "            elif len(parts) > 13:\n",
    "                parts = parts[:13]\n",
    "            clean_lines.append(' '.join(parts))\n",
    "    return clean_lines\n",
    "\n",
    "try:\n",
    "    clean_data = preprocess_data(df1_file)\n",
    "    clean_data_str = '\\n'.join(clean_data)\n",
    "    df = pd.read_csv(StringIO(clean_data_str), sep='\\s+', header=None, dtype=str)\n",
    "\n",
    "    # Assuming that column indexes are correctly targeted, if column 10 can have identifiers, clean it\n",
    "    df[10] = df[10].apply(lambda x: np.nan if (isinstance(x, str) and len(x) > 4) else x)\n",
    "    df[10] = pd.to_numeric(df[10], errors='coerce')\n",
    "\n",
    "    # Define which columns are Year, Month, Day, Magnitude based on 0-index\n",
    "    df.columns = ['Col0', 'Year', 'Month', 'Day', 'Col4', 'Col5', 'Col6', 'Col7', 'Col8', 'Col9', 'Magnitude', 'Col11', 'Col12']\n",
    "\n",
    "    df1 = df[['Year', 'Month', 'Day', 'Magnitude']].copy()  # Explicit copy to work safely\n",
    "\n",
    "    # Apply transformations using .loc to avoid SettingWithCopyWarning\n",
    "    df1.loc[:, 'Year'] = df1['Year'].astype(str).str.extract('(\\d{4})')[0]\n",
    "    df1.loc[:, 'Month'] = df1['Month'].astype(str).str.zfill(2)\n",
    "    df1.loc[:, 'Day'] = df1['Day'].astype(str).str.split('.').str[0].str.zfill(2)\n",
    "\n",
    "    # Creating the 'DateTime' column properly\n",
    "    df1.loc[:, 'DateTime'] = pd.to_datetime(df1['Year'] + df1['Month'] + df1['Day'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "    # Convert Magnitude column to numeric, handling non-numeric values by setting them to NaN\n",
    "    df1.loc[:, 'Magnitude'] = pd.to_numeric(df1['Magnitude'], errors='coerce')\n",
    "\n",
    "    # Clean dataframe from NaN values properly\n",
    "    df1.dropna(subset=['Magnitude', 'DateTime'], inplace=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error reading MPC file: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f83c1-b9fb-49b5-9162-02693f08e977",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Check if the 'Year' column contains any non-null values\n",
    "try:\n",
    "    df1['Year'] = df1['Year'].str.replace(r'\\D+', '', regex=True).astype(str)\n",
    "    df1['Month'] = df1['Month'].astype(str).str.zfill(2)\n",
    "    df1['Day'] = df1['Day'].astype(str).str.zfill(2)\n",
    "\n",
    "    # Attempt to parse datetime\n",
    "    df1['DateTime'] = pd.to_datetime(df1['Year'] + df1['Month'] + df1['Day'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "    # Further operations\n",
    "    df1['DateTime'] = pd.to_datetime(df1['DateTime'], errors='coerce')\n",
    "    df1['Magnitude'] = pd.to_numeric(df1['Magnitude'], errors='coerce')\n",
    "    df1.dropna(subset=['Magnitude', 'DateTime'], inplace=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to process the MCP data:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fdefea-9a18-4f27-a3d8-993137155461",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Read the second data file\n",
    "df2 = pd.read_csv(df2_file, sep='\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58781a-eda3-48e9-9ee7-72ccf1609810",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine date and time from the second data file\n",
    "df2['DateTime'] = pd.to_datetime(df2['Date'] + ' ' + df2['Time'])\n",
    "\n",
    "# Convert the 'DateTime' column to datetime data type with a specified format\n",
    "df2['DateTime'] = pd.to_datetime(df2['DateTime'], format='%Y-%m-%d')\n",
    "\n",
    "# Remove rows with NaT values in DateTime column\n",
    "df2 = df2.dropna(subset=['DateTime'])\n",
    "\n",
    "# Choose which column to graph based on the column header\n",
    "column_to_plot = COLUMN_TO_PLOT  # Change this to plot different columns in the variable section\n",
    "\n",
    "# Remove rows with NaN values in column_to_plot column\n",
    "df2 = df2.dropna(subset=[column_to_plot])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd068de-b10f-4c0a-bceb-0453bf75b2d8",
   "metadata": {},
   "source": [
    "## Plot\n",
    "After the notebook has run, you should see a plot in this section. After reviewing it, you may determine that more work will needed and [variables](#variables) may need modified.\n",
    "\n",
    "See [matplotlib docs](https://matplotlib.org/stable/api/markers_api.html) for marker types.\n",
    "\n",
    "### When things are not perfect\n",
    "\n",
    "1. Breathe\n",
    "2. Get a cup of coffee or tea\n",
    "3. Check the Y-Axis limits in variables section. If the limits are wonky, the displayed results will look wrong.\n",
    "4. Add print statements for variables\n",
    "5. Check the data files for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d8f56-2d7c-4b35-a453-bd00f397eb2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Here we combine the pd objects df1 and df2 into one new object df3 so that we can plot all the data on the same graph\n",
    "\n",
    "# Select relevant columns from df2\n",
    "if ERROR_COLUMN:\n",
    "    df2_selected = df2[['DateTime', column_to_plot, ERROR_COLUMN]]\n",
    "else:\n",
    "    df2_selected = df2[['DateTime', column_to_plot]]\n",
    "\n",
    "try:\n",
    "    # Combine the columns from the two data file into a new thrid data object\n",
    "    df1_selected = df1[['DateTime', 'Magnitude']]\n",
    "    df3 = pd.concat([df1_selected, df2_selected], ignore_index=True)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error reading MPC file: \", e)\n",
    "    df3 = df2_selected\n",
    "\n",
    "# Format the date strings for df3 in MM/DD format\n",
    "if SOHO_ONLY:\n",
    "    date_labels_df3 = df3['DateTime'].dt.strftime('%m/%d %H:%M')\n",
    "else:\n",
    "    date_labels_df3 = df3['DateTime'].dt.strftime('%m/%d')\n",
    "\n",
    "# Calculate the range of dates in the df3 dataset to scale X-Axis\n",
    "date_range = df3['DateTime'].max() - df3['DateTime'].min()\n",
    "\n",
    "# Calculate the step size for each tick on the X-Axis based on XSTEPS\n",
    "step_size = date_range / (XSTEPS - 1)\n",
    "\n",
    "# Generate the tick positions for the X-Axis\n",
    "tick_positions = [df3['DateTime'].min() + step_size * i for i in range(XSTEPS)]\n",
    "\n",
    "# Generate the tick labels for the X-Axis\n",
    "if SOHO_ONLY:\n",
    "    tick_labels = [date.strftime('%m/%d %H:%M') for date in tick_positions]\n",
    "else:\n",
    "    tick_labels = [date.strftime('%m/%d') for date in tick_positions]\n",
    "\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "plt.xticks(tick_positions, tick_labels, rotation=XROT)\n",
    "\n",
    "# Set the y-axis limits manually\n",
    "plt.ylim(MIN_MAG, MAX_MAG)  # Adjust the limits as needed in the variable section\n",
    "\n",
    "# Scatter plot for df3, specifying marker and color for each dataset\n",
    "if SOHO_ONLY:\n",
    "    scatter = plt.scatter(df3['DateTime'], df3[column_to_plot], label=DF2_LABEL, s=15, edgecolor='green', marker='^', facecolor='green')\n",
    "    if ERROR_COLUMN:\n",
    "        plt.errorbar(df3['DateTime'], df3[column_to_plot], yerr=df3[ERROR_COLUMN], fmt='none', ecolor='green', capsize=1, linewidth=0.5)\n",
    "else:\n",
    "    try:\n",
    "        plt.scatter(df3['DateTime'], df3['Magnitude'], label=DF1_LABEL, s=10, edgecolor='red', marker='o', facecolor='none')\n",
    "        scatter = plt.scatter(df3['DateTime'], df3[column_to_plot], label=DF2_LABEL, s=15, edgecolor='green', marker='^', facecolor='green')\n",
    "        if ERROR_COLUMN:\n",
    "            plt.errorbar(df3['DateTime'], df3[column_to_plot], yerr=df3[ERROR_COLUMN], fmt='none', ecolor='green', capsize=1, linewidth=0.5)\n",
    "    except Exception as e:\n",
    "        print(\"Error printing MPC data: \", e)\n",
    "\n",
    "# Add labels for the x and y axes\n",
    "plt.xlabel(XLABEL)\n",
    "plt.ylabel(YLABEL)\n",
    "\n",
    "# Add title\n",
    "plot_title = COMET + ' - ' + YEAR\n",
    "plt.title(plot_title)\n",
    "\n",
    "# Change the font size of tick labels\n",
    "plt.xticks(fontsize=8)  # Adjust the font size as needed\n",
    "plt.yticks(fontsize=8)  # Adjust the font size as needed\n",
    "\n",
    "# Invert the Y-axis so smaller numbers represent brighter magnitudes\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "#Create the legend and allow python to put it in the most reasonable place\n",
    "if DF2_LABEL:\n",
    "    plt.legend()\n",
    "\n",
    "# Draw the plot\n",
    "#plt.show()\n",
    "\n",
    "# Save the plot; rename to desired filename\n",
    "if ERROR_COLUMN:\n",
    "    filename_base = YEAR + '_' + COMET.replace(\" \", \"\") + '-ERR' + '.png'\n",
    "else:\n",
    "    filename_base = YEAR + '_' + COMET.replace(\" \", \"\") + '.png'\n",
    "if SOHO_ONLY:\n",
    "    filename = 'SOHO_' + filename_base\n",
    "else:\n",
    "    filename ='MPC_SOHO_' + filename_base\n",
    "\n",
    "filename = filename.replace(\"/\", \"_\")\n",
    "plt.savefig(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101ffa3-3e4e-4a8a-acfa-68fa5e67024b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
